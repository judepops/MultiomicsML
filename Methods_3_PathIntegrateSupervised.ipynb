{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import sklearn \n",
    "import sspa\n",
    "import sspa.utils\n",
    "import gseapy.plot as gp\n",
    "import networkx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import urllib.request\n",
    "import statsmodels\n",
    "import networkx as nx\n",
    "import math\n",
    "import itertools \n",
    "from scipy.stats import hypergeom as hg\n",
    "import textwrap\n",
    "from itertools import chain\n",
    "import missforest\n",
    "import pathintegrate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Loading in data - HERE WE ARE PERFORMING PATHINTEGRATE SUPERVISED AT DIFFERENT THRESHOLDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw metabolomics data (this is used for generating different ID - converted matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the metabolomics data\n",
    "metabolomics_data_processed = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Processing/Processing_Cleaned/cleaned_metabolomics_data_covid.csv')\n",
    "metabolomics_data_processed = metabolomics_data_processed.set_index('sample_id')\n",
    "metabolomics_data_processed_final = metabolomics_data_processed.iloc[:, :-7]\n",
    "metabolomics_data_processed_final.columns = [col.strip().lower() for col in metabolomics_data_processed_final.columns]\n",
    "\n",
    "last_7_columns = metabolomics_data_processed.iloc[:, -7:]\n",
    "last_7_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm ID predictions: subset to metabolites that have manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_subset = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/LLM_Annotation/manual_automated_subset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating llm predictions at different threshold cutoffs: 0.5 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify the thresohld columns\n",
    "threshold_columns = [col for col in llm_subset.columns if 'Matched COMPOUND_ID' in col]\n",
    "\n",
    "# unpivot the dataframe using melting\n",
    "llm_melted = pd.melt(llm_subset, \n",
    "                    id_vars=['Query'],\n",
    "                    value_vars=threshold_columns,\n",
    "                    var_name='Threshold',\n",
    "                    value_name='ChEBI')\n",
    "\n",
    "# cleaning the threshold column - removing any whitespace that can cause spurious matching\n",
    "llm_melted['Threshold'] = llm_melted['Threshold'].str.extract(r'(\\d+\\.\\d+)$').astype(float)\n",
    "llm_melted = llm_melted.dropna(subset=['Threshold'])\n",
    "llm_melted.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# removing any columns that dont have chebi ID predictions (na columns)\n",
    "llm_melted = llm_melted.dropna(subset=['ChEBI'])\n",
    "\n",
    "# making chebi correct\n",
    "llm_melted['ChEBI'] = pd.to_numeric(llm_melted['ChEBI'], errors='coerce')\n",
    "llm_melted['ChEBI'] = llm_melted['ChEBI'].astype('Int64')\n",
    "\n",
    "# getting unique threshold values\n",
    "unique_thresholds = llm_melted['Threshold'].unique()\n",
    "\n",
    "# now this is a dciontary to hold the individual dataframes created by each threshold subset\n",
    "threshold_dfs = {}\n",
    "for threshold in unique_thresholds:\n",
    "    threshold_df = llm_melted[llm_melted['Threshold'] == threshold].copy()\n",
    "    threshold_dfs[f\"llm_{threshold}\"] = threshold_df\n",
    "\n",
    "# printing the keys to visualise these dataframes\n",
    "print(threshold_dfs.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we map the raw metabolomics data to ChEBI IDs using the different threshold prediction dataframes as the conversion reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple ID - converted matrices are created (each with a differnt number of IDs due to differetn LLM thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the downloaded multi-omics reactome pathway database \n",
    "mo_paths_reactome = sspa.process_gmt(infile='Reactome_Homo_sapiens_pathways_multiomics_R89.gmt')\n",
    "\n",
    "# converign the dataframe to object type\n",
    "mo_paths_reactome = mo_paths_reactome.astype('object')\n",
    "\n",
    "# intialising a dictionary to hold  processed data mapped for each threshold\n",
    "processed_data_mapped_dict = {}\n",
    "\n",
    "# looping through each threshold dataframe\n",
    "for threshold, df in threshold_dfs.items():\n",
    "    # running provided commands on each dataframe\n",
    "    df = df.drop(columns='Threshold')\n",
    "    processed_data_mapped = sspa.map_identifiers(df, output_id_type=\"ChEBI\", matrix=metabolomics_data_processed_final)\n",
    "    processed_data_mapped.columns = processed_data_mapped.columns.map(str)\n",
    "    \n",
    "    all_reactome_cpds = set(sum(sspa.utils.pathwaydf_to_dict(mo_paths_reactome).values(), []))\n",
    "    mapped_annotated_cpds_met = set(processed_data_mapped.columns) & set(all_reactome_cpds)\n",
    "\n",
    "    last_7_columns = metabolomics_data_processed.iloc[:, -7:]\n",
    "    processed_data_mapped_final = pd.concat([processed_data_mapped, last_7_columns], axis=1)\n",
    "    \n",
    "    # storing the mapped processed data as a dictionary\n",
    "    processed_data_mapped_dict[threshold] = processed_data_mapped_final\n",
    "\n",
    "    \n",
    "# subsetting the dictionary to only include thresholds that are 0,5 or greater (there is no change in compounds before 0.5)\n",
    "subset_processed_data = {threshold: data for threshold, data in processed_data_mapped_dict.items() if float(threshold.split('_')[-1]) >= 0.5}\n",
    "\n",
    "# printing the contents of the datafame to verify\n",
    "for threshold, data in subset_processed_data.items():\n",
    "    print(f\"Threshold: {threshold}, Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a PathIntegrate cross-validated single-view model to each threshold ID-converted matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic regression function is chosen, aswell as the SVD ssPA method. Pathways are only used at a minimum of 4 compounds per pathway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code takes around 3 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing key dependencies for the machine learnign model\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import interpolate\n",
    "\n",
    "# THIS IS A CONDENSED VERSION OF THE CODE AVAILABLE IN SECTION 2 STANDARD PATHINTEGRATE WORKFLOW - \n",
    "# THIS IS JUST A FUNCTION TO RUN THE SINGLEVIEW LOGISTIC REGRESSION CROSS-VALIDATED MODEL ON EVER \n",
    "# SINGLE DIFFERENT ID-CONVERTED MATRXI IN ORDER TO DETERMINE THE BEST ID-CONVERTED MATRIX FOR PATINET CLASSIFICATION\n",
    "\n",
    "\n",
    "# FUNCITON 1: CREATING CROSS-VALIDATED MODEL (SEE SECTION 2)\n",
    "\n",
    "def train_and_evaluate_model(random_seed, prot, metab, mo_paths, shuffle_labels=False):\n",
    "    X_train_prot, X_test_prot, y_train, y_test = train_test_split(\n",
    "        prot.drop(columns=['Condition_Group']), prot['Condition_Group'],\n",
    "        test_size=0.33, random_state=random_seed, stratify=prot['Condition_Group']\n",
    "    )\n",
    "\n",
    "    if shuffle_labels:\n",
    "        np.random.shuffle(y_train.values)\n",
    "        np.random.shuffle(y_test.values)\n",
    "\n",
    "    X_train_met, X_test_met = metab.loc[X_train_prot.index, :], metab.loc[X_test_prot.index, :]\n",
    "\n",
    "    pi_model = pathintegrate.PathIntegrate(\n",
    "        omics_data={'Metabolomics_train': X_train_met, 'Proteomics_train': X_train_prot},\n",
    "        metadata=y_train,\n",
    "        pathway_source=mo_paths,\n",
    "        sspa_scoring=sspa.sspa_SVD,\n",
    "        min_coverage=6\n",
    "    )\n",
    "\n",
    "    cv_single_view = pi_model.SingleViewCV(\n",
    "        LogisticRegression,\n",
    "        model_params={'random_state': 0, 'max_iter': 500},\n",
    "        cv_params={'cv': 5, 'scoring': 'f1', 'verbose': 2}\n",
    "    )\n",
    "\n",
    "    print('Mean cross-validated F1 score: ', np.mean(cv_single_view))\n",
    "\n",
    "    sv_tuned = pi_model.SingleView(\n",
    "        model=LogisticRegression,\n",
    "        model_params={'C': 21.54434690031882, 'random_state': 0, 'max_iter': 500}\n",
    "    )\n",
    "\n",
    "    concat_data = pd.concat({'Metabolomics_test': X_test_met, 'Proteomics_test': X_test_prot.iloc[:, :-1]}.values(), axis=1)\n",
    "\n",
    "    pipe_sv = Pipeline([\n",
    "        ('Scaler', StandardScaler().set_output(transform=\"pandas\")),\n",
    "        ('sspa', pi_model.sspa_method(pi_model.pathway_source, pi_model.min_coverage)),\n",
    "    ])\n",
    "\n",
    "    test_set_scores = pipe_sv.fit_transform(concat_data)\n",
    "\n",
    "    sv_pred = sv_tuned.predict(test_set_scores)\n",
    "    sv_pred_prob = sv_tuned.predict_proba(test_set_scores)[:, 1]\n",
    "\n",
    "    test_set_f1 = f1_score(y_test, sv_pred)\n",
    "    test_set_precision = precision_score(y_test, sv_pred)\n",
    "    test_set_recall = recall_score(y_test, sv_pred)\n",
    "    test_set_auc = roc_auc_score(y_test, sv_pred_prob)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, sv_pred_prob)\n",
    "\n",
    "    return test_set_f1, test_set_precision, test_set_recall, test_set_auc, fpr, tpr, random_seed\n",
    "\n",
    "\n",
    "# FUNCTION 2: RUNNIGN THE MODEL FOR 50 DIFFERENT RANDOM SEEDS AND GETTING THE AVERAGE ROC \n",
    "# this function also saves key metrics to be used for plotting and also for evaluatign different matrices\n",
    "\n",
    "def run_model_for_all_seeds(random_seeds, prot, metab, mo_paths, shuffle_labels=False):\n",
    "    num_runs = len(random_seeds)\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    auc_scores = []\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    all_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        random_seed = random_seeds[i]\n",
    "        f1, precision, recall, auc, fpr, tpr, used_seed = train_and_evaluate_model(random_seed, prot, metab, mo_paths, shuffle_labels)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        auc_scores.append(auc)\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(interpolate.interp1d(fpr, tpr)(all_fpr))\n",
    "        print(f\"Run {i + 1}: F1 = {f1}, Precision = {precision}, Recall = {recall}, AUC = {auc}, Seed = {used_seed}\")\n",
    "\n",
    "    mean_tpr = np.mean(tpr_list, axis=0)\n",
    "    std_tpr = np.std(tpr_list, axis=0)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "\n",
    "    return all_fpr, mean_tpr, mean_auc, std_tpr, std_auc\n",
    "\n",
    "# generating 50 random seeds to run the model\n",
    "random_seeds = np.random.randint(200, size=50)\n",
    "\n",
    "# loading the pathway data for Reactome - chebi IDs map to the reactome pathway database\n",
    "\n",
    "mo_paths_reactome = sspa.process_gmt(infile='Reactome_Homo_sapiens_pathways_multiomics_R89.gmt')\n",
    "mo_paths_reactome = mo_paths_reactome.astype('object')\n",
    "\n",
    "# the function to process datasets and run models\n",
    "\n",
    "def process_and_run_model(metab, prot_file, mo_paths, random_seeds):\n",
    "    prot = pd.read_csv(prot_file)\n",
    "    prot.set_index('sample_id', inplace=True)\n",
    "    metab.set_index('sample_id', inplace=True)\n",
    "    prot = prot.drop(columns=['Who', 'Race', 'Age', 'Group', 'Age_Group', 'Race_Group'])\n",
    "    metab = metab.drop(columns=['Who', 'Race', 'Age', 'Group', 'Age_Group', 'Race_Group'])\n",
    "    common_indices = prot.index.intersection(metab.index)\n",
    "    prot = prot.loc[common_indices]\n",
    "    metab = metab.loc[common_indices]\n",
    "    metab = metab.iloc[:, :-1]\n",
    "    prot['Condition_Group'] = prot['Condition_Group'].map({'Severe': 1, 'Mild': 0})\n",
    "    \n",
    "    return run_model_for_all_seeds(random_seeds, prot, metab, mo_paths)\n",
    "\n",
    "# proteomics data pathway to be loaded in and integrate diwht the different metabolomics data thresholds\n",
    "prot_file = '/Users/judepops/Documents/PathIntegrate/Code/Pathway_Analysis/COVID_Pro_UniProt_Final.csv'\n",
    "\n",
    "# these are the thresholds to process - we didnt want to run all of them as this took too logn and the lower thresholds \n",
    "# are uninformative as they will just yield the same results\n",
    "\n",
    "selected_thresholds = ['llm_0.6', 'llm_0.7', 'llm_0.75', 'llm_0.8', 'llm_0.9', 'llm_1.0']\n",
    "\n",
    "# dictionary to hold the results for each selected threshold - we can check this later\n",
    "results_dict = {}\n",
    "\n",
    "# looping through each version of metabolomics data in the processed_data_mapped_dict dictionary we created earlier\n",
    "for threshold, data in subset_processed_data.items():\n",
    "    if threshold in selected_thresholds:\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # processing and running the model for each selected threshold\n",
    "        all_fpr, mean_tpr, mean_auc, std_tpr, std_auc = process_and_run_model(data, prot_file, mo_paths_reactome, random_seeds)\n",
    "        \n",
    "        # storing the results\n",
    "        results_dict[threshold] = {\n",
    "            'all_fpr': all_fpr,\n",
    "            'mean_tpr': mean_tpr,\n",
    "            'mean_auc': mean_auc,\n",
    "            'std_tpr': std_tpr,\n",
    "            'std_auc': std_auc\n",
    "        }\n",
    "\n",
    "# plotting ROC curves\n",
    "plt.figure()\n",
    "\n",
    "# creating distrinct colours with seaborn\n",
    "colors = sns.color_palette(\"hsv\", len(results_dict))\n",
    "\n",
    "# Loop through the results and plot each one\n",
    "for i, (threshold, results) in enumerate(results_dict.items()):\n",
    "    plt.plot(results['all_fpr'], results['mean_tpr'], color=colors[i], \n",
    "             label=f'Threshold {threshold} ROC curve (area = {results[\"mean_auc\"]:.2f}, std = {results[\"std_auc\"]:.2f})')\n",
    "    plt.fill_between(results['all_fpr'], results['mean_tpr'] - results['std_tpr'], \n",
    "                     results['mean_tpr'] + results['std_tpr'], color=colors[i], alpha=0.2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('roc curves for diff thresholds with standard deviation')\n",
    "plt.legend(loc=\"lower right\", prop={'size': 8})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating and saving a plot with desired colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_thresholds = ['llm_0.6', 'llm_0.7', 'llm_0.75','llm_0.8', 'llm_0.9', 'llm_1.0']\n",
    "custom_palette = [\"red\", \"blue\", \"green\", 'brown', \"orange\", \"purple\"]\n",
    "\n",
    "# intiialising the figure\n",
    "plt.figure()\n",
    "\n",
    "# looping through the results for selected thresholds and plotting each one\n",
    "for i, threshold in enumerate(selected_thresholds):\n",
    "    results = results_dict[threshold]\n",
    "    # ensuring the curve starts at (0, 0)\n",
    "    if results['all_fpr'][0] != 0 or results['mean_tpr'][0] != 0:\n",
    "        results['all_fpr'] = np.insert(results['all_fpr'], 0, 0)\n",
    "        results['mean_tpr'] = np.insert(results['mean_tpr'], 0, 0)\n",
    "\n",
    "    # plotting as a step plot\n",
    "    plt.step(results['all_fpr'], results['mean_tpr'], where='post', color=custom_palette[i], \n",
    "             label=f'Threshold {threshold} ROC curve (area = {results[\"mean_auc\"]:.2f}, std = {results[\"std_auc\"]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "# setting the labels and title\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('roc curves for diff thresholds with standard deviation')\n",
    "\n",
    "plt.legend(loc=\"lower right\", prop={'size': 6.5})\n",
    "plt.savefig('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_D/ROC_Threshold_Comparison/roc_curves_final_thresh.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Analysis to Compare Methods - Not thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALl of the dataframes used here were created in Section_3_ComparingMethods.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code is a similar ROC curve to before, with the same parameters but different input datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as last time - function to evaluate and train model\n",
    "def train_and_evaluate_model(random_seed, prot, metab, mo_paths, shuffle_labels=False):\n",
    "    X_train_prot, X_test_prot, y_train, y_test = train_test_split(\n",
    "        prot.drop(columns=['Condition_Group']), prot['Condition_Group'],\n",
    "        test_size=0.33, random_state=random_seed, stratify=prot['Condition_Group']\n",
    "    )\n",
    "\n",
    "    if shuffle_labels:\n",
    "        np.random.shuffle(y_train.values)\n",
    "        np.random.shuffle(y_test.values)\n",
    "\n",
    "    X_train_met, X_test_met = metab.loc[X_train_prot.index, :], metab.loc[X_test_prot.index, :]\n",
    "\n",
    "    pi_model = pathintegrate.PathIntegrate(\n",
    "        omics_data={'Metabolomics_train': X_train_met, 'Proteomics_train': X_train_prot},\n",
    "        metadata=y_train,\n",
    "        pathway_source=mo_paths,\n",
    "        sspa_scoring=sspa.sspa_SVD,\n",
    "        min_coverage=6\n",
    "    )\n",
    "\n",
    "    cv_single_view = pi_model.SingleViewCV(\n",
    "        LogisticRegression,\n",
    "        model_params={'random_state': 0, 'max_iter': 500},\n",
    "        cv_params={'cv': 5, 'scoring': 'f1', 'verbose': 2}\n",
    "    )\n",
    "\n",
    "    print('Mean cross-validated F1 score: ', np.mean(cv_single_view))\n",
    "\n",
    "    sv_tuned = pi_model.SingleView(\n",
    "        model=LogisticRegression,\n",
    "        model_params={'C': 21.54434690031882, 'random_state': 0, 'max_iter': 500}\n",
    "    )\n",
    "\n",
    "    concat_data = pd.concat({'Metabolomics_test': X_test_met, 'Proteomics_test': X_test_prot.iloc[:, :-1]}.values(), axis=1)\n",
    "\n",
    "    pipe_sv = Pipeline([\n",
    "        ('Scaler', StandardScaler().set_output(transform=\"pandas\")),\n",
    "        ('sspa', pi_model.sspa_method(pi_model.pathway_source, pi_model.min_coverage)),\n",
    "    ])\n",
    "\n",
    "    test_set_scores = pipe_sv.fit_transform(concat_data)\n",
    "\n",
    "    sv_pred = sv_tuned.predict(test_set_scores)\n",
    "    sv_pred_prob = sv_tuned.predict_proba(test_set_scores)[:, 1]\n",
    "\n",
    "    test_set_f1 = f1_score(y_test, sv_pred)\n",
    "    test_set_precision = precision_score(y_test, sv_pred)\n",
    "    test_set_recall = recall_score(y_test, sv_pred)\n",
    "    test_set_auc = roc_auc_score(y_test, sv_pred_prob)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, sv_pred_prob)\n",
    "\n",
    "    return test_set_f1, test_set_precision, test_set_recall, test_set_auc, fpr, tpr, random_seed\n",
    "\n",
    "# function to run the model for all seeds and calculate the average roc\n",
    "def run_model_for_all_seeds(random_seeds, prot, metab, mo_paths, shuffle_labels=False):\n",
    "    num_runs = len(random_seeds)\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    auc_scores = []\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    all_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        random_seed = random_seeds[i]\n",
    "        f1, precision, recall, auc, fpr, tpr, used_seed = train_and_evaluate_model(random_seed, prot, metab, mo_paths, shuffle_labels)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        auc_scores.append(auc)\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(interpolate.interp1d(fpr, tpr)(all_fpr))\n",
    "        print(f\"Run {i + 1}: F1 = {f1}, Precision = {precision}, Recall = {recall}, AUC = {auc}, Seed = {used_seed}\")\n",
    "\n",
    "    mean_tpr = np.mean(tpr_list, axis=0)\n",
    "    std_tpr = np.std(tpr_list, axis=0)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "\n",
    "    return all_fpr, mean_tpr, mean_auc, std_tpr, std_auc\n",
    "\n",
    "# generating 50 random seeds to run the model\n",
    "random_seeds = np.random.randint(200, size=50)\n",
    "\n",
    "# Common pathway data for Reactome\n",
    "mo_paths_reactome = sspa.process_gmt(infile='Reactome_Homo_sapiens_pathways_multiomics_R89.gmt')\n",
    "mo_paths_reactome = mo_paths_reactome.astype('object')\n",
    "\n",
    "# function to process datasets and run models prior to the cross valiation adn model\n",
    "def process_and_run_model(metab, prot_file, mo_paths, random_seeds, method_name):\n",
    "    prot = pd.read_csv(prot_file)\n",
    "    prot.set_index('sample_id', inplace=True)\n",
    "    metab.set_index('sample_id', inplace=True)\n",
    "    prot = prot.drop(columns=['Who', 'Race', 'Age', 'Group', 'Age_Group', 'Race_Group'])\n",
    "    metab = metab.drop(columns=['Who', 'Race', 'Age', 'Group', 'Age_Group', 'Race_Group'])\n",
    "    common_indices = prot.index.intersection(metab.index)\n",
    "    prot = prot.loc[common_indices]\n",
    "    metab = metab.loc[common_indices]\n",
    "    metab = metab.iloc[:, :-1]\n",
    "    prot['Condition_Group'] = prot['Condition_Group'].map({'Severe': 1, 'Mild': 0})\n",
    "    \n",
    "    return run_model_for_all_seeds(random_seeds, prot, metab, mo_paths)\n",
    "\n",
    "# loading and processing versions of metabolomics data\n",
    "metab_manual = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_D/COVID_Met_ChEBI_Maual.csv')\n",
    "metab_llm = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_D/COVID_Met_ChEBI_LLM_0.75.csv')\n",
    "metab_met = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_D/COVID_Met_ChEBI_Metaboanalyst.csv')\n",
    "metab_llm_v2 = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_D/COVID_Met_ChEBI_LLM_V2.csv')\n",
    "prot_file = '/Users/judepops/Documents/PathIntegrate/Code/Pathway_Analysis/COVID_Pro_UniProt_Final.csv'\n",
    "\n",
    "print('Starting')\n",
    "\n",
    "# running the mdoel for each dataset\n",
    "fpr_manual, mean_tpr_manual, mean_auc_manual, std_tpr_manual, std_auc_manual = process_and_run_model(metab_manual, prot_file, mo_paths_reactome, random_seeds, 'Manual')\n",
    "print('Done Manual')\n",
    "fpr_llm, mean_tpr_llm, mean_auc_llm, std_tpr_llm, std_auc_llm = process_and_run_model(metab_llm, prot_file, mo_paths_reactome, random_seeds, 'LLM')\n",
    "print('Done LLM')\n",
    "fpr_met, mean_tpr_met, mean_auc_met, std_tpr_met, std_auc_met = process_and_run_model(metab_met, prot_file, mo_paths_reactome, random_seeds, 'Metaboanalyst')\n",
    "print('Done Met')\n",
    "fpr_llm_v2, mean_tpr_llm_v2, mean_auc_llm_v2, std_tpr_llm_v2, std_auc_llm_v2 = process_and_run_model(metab_llm_v2, prot_file, mo_paths_reactome, random_seeds, 'LLM V2')\n",
    "print('Done LLM V2')\n",
    "\n",
    "# plottign the roc curves\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr_manual, mean_tpr_manual, color='blue', label=f'Manual (area = {mean_auc_manual:.2f}, std = {std_auc_manual:.2f})')\n",
    "\n",
    "plt.plot(fpr_llm, mean_tpr_llm, color='red', label=f'LLM 0.75 V1 (area = {mean_auc_llm:.2f}, std = {std_auc_llm:.2f})')\n",
    "\n",
    "plt.plot(fpr_met, mean_tpr_met, color='green', label=f'Metaboanalyst (area = {mean_auc_met:.2f}, std = {std_auc_met:.2f})')\n",
    "\n",
    "plt.plot(fpr_llm_v2, mean_tpr_llm_v2, color='magenta', label=f'LLM 0.75 V2 (area = {mean_auc_llm_v2:.2f}, std = {std_auc_llm_v2:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Different Metabolomics Methods with Standard Deviation')\n",
    "plt.legend(loc=\"lower right\", prop={'size': 8})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a ROC curve plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_start_at_zero(fpr, tpr):\n",
    "    if fpr[0] != 0 or tpr[0] != 0:\n",
    "        fpr = np.insert(fpr, 0, 0)\n",
    "        tpr = np.insert(tpr, 0, 0)\n",
    "    return fpr, tpr\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "fpr_met, mean_tpr_met = ensure_start_at_zero(fpr_met, mean_tpr_met)\n",
    "plt.step(fpr_met, mean_tpr_met, where='post', color='orange', label=f'Metaboanalyst (area = {mean_auc_met:.2f}, std = {std_auc_met:.2f})')\n",
    "fpr_llm, mean_tpr_llm = ensure_start_at_zero(fpr_llm, mean_tpr_llm)\n",
    "plt.step(fpr_llm, mean_tpr_llm, where='post', color='blue', label=f'0.75 LLM v1 (area = {mean_auc_llm:.2f}, std = {std_auc_llm:.2f})')\n",
    "fpr_llm_v2, mean_tpr_llm_v2 = ensure_start_at_zero(fpr_llm_v2, mean_tpr_llm_v2)\n",
    "plt.step(fpr_llm_v2, mean_tpr_llm_v2, where='post', color='red', label=f'0.75 LLM v2 (area = {mean_auc_llm_v2:.2f}, std = {std_auc_llm_v2:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('roc Curves for diff annotation methods')\n",
    "plt.legend(loc=\"lower right\", prop={'size': 9})\n",
    "\n",
    "plt.tck_params(axis='both', which='major', labelsize=13)\n",
    "\n",
    "#save\n",
    "#plt.savefig('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_D/ROC_Reactome_Method_Comparison/roc_curves_met_FINAL_NoLLMMet.png', dpi=500)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
