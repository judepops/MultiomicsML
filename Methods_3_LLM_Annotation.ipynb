{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up ElasticSearch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", '{private key}'),\n",
    "    ca_certs=\"/Users/judepops/Documents/PathIntegrate/Code/Processing/semantic_search/elasticsearch-8.13.2/config/certs/http_ca.crt\"\n",
    ")\n",
    "\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = es.indices.delete(index='compounds')\n",
    "    print(\"Index deleted:\", response)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "response = es.indices.clear_cache(index='*')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the ChEBI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('names.tsv', sep=\"\\t\") # this is the dataframe from ChEBI which contains 400,000 manual chebi IDs\n",
    "df.head()\n",
    "\n",
    "## Prepare the data\n",
    "df.isna().value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting to the key columns of interest\n",
    "df = df[['NAME', 'COMPOUND_ID', 'TYPE', 'SOURCE']]\n",
    "\n",
    "has_na = df.isna().any().any()  \n",
    "\n",
    "if has_na:\n",
    "    print(\"Has\")\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting name to string and filling any missing wtih unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NAME'] = df['NAME'].fillna('unknown').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure there are no duplicate entries: if there are, we keep the KEGG COMPOUND entry (LLM V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatign a fucntion to decide which entry to priortise in the case of duplicates\n",
    "\n",
    "def prioritise_duplicates(df):\n",
    "    source_priority = {'KEGG COMPOUND': 1, 'ChEBI': 2}\n",
    "    df['SOURCE_PRIORITY'] = df['SOURCE'].map(source_priority).fillna(3)\n",
    "    df.sort_values(by=['NAME', 'SOURCE_PRIORITY'], inplace=True)\n",
    "    df = df.drop_duplicates(subset='NAME', keep='first')\n",
    "    df.drop(columns=['SOURCE_PRIORITY'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# applying  function\n",
    "df = prioritise_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing any compound names that are longer than 100 bytes - these are extremely long compound that simply dont exist in our data so there is no point indxing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_sizes = df['NAME'].apply(lambda x: len(str(x).encode('utf-8')))\n",
    "df = df[byte_sizes <= 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, importing the pre-trained BERT model and converting the Name column into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding each name into vector representation and saving it in vector column - this takes around 7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NAME_VECTOR'] = df['NAME'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving file to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector is a multidimensional array of numbers\n",
    "df.to_pickle('KEGG_ChEBI_vectors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new index in the ElasticSearch server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletign the current index to make sure nothign is there\n",
    "try:\n",
    "    response = es.indices.delete(index='compounds')\n",
    "    print(\" deleted:\", response)\n",
    "except Exception as e:\n",
    "    print(\" error :\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An index mapping file is used for the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the index - indexMapping.py in same directory\n",
    "from indexMapping import indexMapping\n",
    "es.indices.create(index=\"compounds\", mappings=indexMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index mapping file (for visualisation puurposes:\n",
    "\n",
    "indexMapping = {\n",
    "    \"properties\":{\n",
    "        \"COMPOUND_ID\":{\n",
    "            \"type\":\"long\"\n",
    "        },\n",
    "        \"TYPE\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"SOURCE\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"NAME_VECTOR\":{\n",
    "            \"type\":\"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\": True,\n",
    "            \"similarity\":\"l2_norm\"\n",
    "        },\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting the original dataframe to a dictionary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df.to_dict(\"records\") # this is a parameter in to_dict taht specifies the orientation of the returned dictionary in a JSON format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to print the indexmapping we added to ealstic search just to ameks ure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_index_mapping(index_name):\n",
    "    try:\n",
    "        mapping = es.indices.get_mapping(index=index_name)\n",
    "        print(mapping[index_name]['mappings'])\n",
    "    except Exception as e:\n",
    "        print(\"error\", e)\n",
    "print_index_mapping(\"compounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now, ingest each df_dict entry into ElasticSearhc, storing them under their name (compound names) - this takes around 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in record_list:\n",
    "    try:\n",
    "        es.index(index=\"compounds\", document=record, id=record[\"NAME\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the elasticsearch index for an example compound spermidine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifiying compound to search\n",
    "input_keyword = 'spermidine'\n",
    "vector_of_input_keyword = model.encode(input_keyword)\n",
    "\n",
    "query = {\n",
    "    \"field\": \"NAME_VECTOR\", # what we are searching against (the query is converted into a vector)\n",
    "    \"query_vector\": vector_of_input_keyword,\n",
    "    \"k\": 4,\n",
    "    \"num_candidates\" : 10000, #the database size\n",
    "}\n",
    "\n",
    "res = es.knn_search(index='compounds', knn=query, source=['COMPOUND_ID', 'NAME', 'TYPE'])\n",
    "res[\"hits\"][\"hits\"] # accsesing resuts from the output JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can run this on all covid compounds in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laoding in the original, cleaned metabolomics data\n",
    "metabolomics_data_original = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Processing/Processing_Cleaned/cleaned_metabolomics_data_covid.csv')\n",
    "\n",
    "# extracing the compound names\n",
    "metabolomics_data_original.set_index('sample_id', inplace=True)\n",
    "metabolomics_data = metabolomics_data_original.iloc[:, :-7]\n",
    "metabolomics_data.columns = [col.strip().lower() for col in metabolomics_data.columns]\n",
    "metabolomics_data\n",
    "\n",
    "# retrieving the names and makign a dataframe\n",
    "column_names = metabolomics_data.columns.tolist()\n",
    "covid_compounds = pd.DataFrame(column_names, columns=['Compound Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funciton that performs the mapping of input covid compounds into their ChEBI IDs - additional parameters are added for LLM v2: kegg compound is selected and scores must be above 0.75 for matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we initialise an output list to store the results\n",
    "output_records = []\n",
    "\n",
    "# looping through each compound in the dataframe\n",
    "for compound_name in covid_compounds['Compound Name']:\n",
    "    # encoding the compound name to a vector\n",
    "    vector_of_compound_name = model.encode(compound_name)\n",
    "\n",
    "    # defining the knn query for searching - we use the vectors to match l2norm\n",
    "    query = {\n",
    "        \"field\": \"NAME_VECTOR\",\n",
    "        \"query_vector\": vector_of_compound_name,\n",
    "        \"k\": 5,  # this returns the top 5 matches using KNN search\n",
    "        \"num_candidates\": 10000 # the maximum fo 10000 candidates are checked each time\n",
    "    }\n",
    "\n",
    "    #  knn search in elasticsearch\n",
    "    res = es.knn_search(index='compounds', knn=query, _source=['COMPOUND_ID', 'NAME', 'TYPE', 'SOURCE'])\n",
    "\n",
    "    # getting the actual results from the search - extracting from teh JSON file\n",
    "    hits = res[\"hits\"][\"hits\"]\n",
    "\n",
    "    # initialising variables to store  best matches\n",
    "    best_hit = None\n",
    "    best_score = -np.inf\n",
    "    best_is_kegg = False  # tracking if best match is a kegg compound - important for llm v2\n",
    "\n",
    "    # going through each hit to skip if score is not greater than 0.75 (this was added after threshold anlaysis to avoid poor compounds)\n",
    "    for hit in hits:\n",
    "        score = hit['_score']  \n",
    "        if score <= 0.75:\n",
    "            continue  # skipping if the score isnt above 0.75\n",
    "\n",
    "        compound_id = str(hit['_source']['COMPOUND_ID']).strip()  # making sure compound_id is a string and stripped from confoudnign characters\n",
    "        source = hit['_source'].get('SOURCE')\n",
    "        type = hit['_source'].get('TYPE')\n",
    "        is_kegg = source == 'KEGG COMPOUND'\n",
    "\n",
    "        # deciding if the hit is better than the current best one\n",
    "        if (best_hit is None) or (score > best_score) or \\\n",
    "           (score == best_score and is_kegg and not best_is_kegg):\n",
    "            best_hit = hit\n",
    "            best_score = score\n",
    "            best_is_kegg = is_kegg\n",
    "\n",
    "    # taking the top match for each compound\n",
    "    if best_hit:  # check if we found a good match\n",
    "        matched_name = best_hit['_source']['NAME']\n",
    "        matched_compound_id = best_hit['_source']['COMPOUND_ID']\n",
    "\n",
    "        # adding the result to the output list\n",
    "        output_records.append({\n",
    "            'Input Compound Name': compound_name,\n",
    "            'Matched Compound Name': matched_name,\n",
    "            'Matched COMPOUND_ID': matched_compound_id,\n",
    "            'Source': best_hit['_source']['SOURCE']\n",
    "        })\n",
    "\n",
    "# reate a dataframe from the output records list that we created\n",
    "output_df = pd.DataFrame(output_records)\n",
    "output_df.to_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_B/0.75_KEGG_Elastic_automated.csv', index=False) # saving the final file which is the llm v2 fle in this case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettign the results are a new dataframe\n",
    "scores = output_df.copy()\n",
    "\n",
    "scores['Matched COMPOUND_ID'] = pd.to_numeric(scores['Matched COMPOUND_ID'], errors='coerce')\n",
    "if scores['Matched COMPOUND_ID'].isnull().any():\n",
    "    scores['Matched COMPOUND_ID'] = scores['Matched COMPOUND_ID'].fillna(0).astype(int)\n",
    "else:\n",
    "    scores['Matched COMPOUND_ID'] = scores['Matched COMPOUND_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsetting the results to IDs that are in the manual dataframe - have IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_id = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Manual_Annotation/manual_annotations_raw_final_2.csv', index_col=0)\n",
    "\n",
    "columns_to_drop = ['Automated_Match', 'Automated_ChEBI', 'Confusion_Matrix', 'Unnamed: 4', 'Manual_Match', 'Input Compound Name']\n",
    "correct_id = correct_id.drop(columns=columns_to_drop)\n",
    "\n",
    "correct_id.rename(columns={'Manual_ChEBI': 'ChEBI'}, inplace=True)\n",
    "\n",
    "if correct_id['ChEBI'].isnull().any():\n",
    "    correct_id['ChEBI'] = correct_id['ChEBI'].fillna(0).astype(int)\n",
    "else:\n",
    "    correct_id['ChEBI'] = correct_id['ChEBI'].astype(int)\n",
    "\n",
    "correct_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping whitespace and converting to lower case for both dataframes\n",
    "scores['Input Compound Name'] = scores['Input Compound Name'].str.strip().str.lower()\n",
    "correct_id['Compound Name'] = correct_id['Compound Name'].str.strip().str.lower()\n",
    "# renaing \n",
    "correct_id.rename(columns={'Compound Name': 'Query'}, inplace=True)\n",
    "scores.rename(columns={'Input Compound Name': 'Query'}, inplace=True)\n",
    "# merging the data \n",
    "merged_df = scores.merge(correct_id, on='Query', how='left')\n",
    "# renaming columns\n",
    "merged_df.rename(columns={'ChEBI': 'Correct COMPOUND_ID'}, inplace=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Matched COMPOUND_ID'] = pd.to_numeric(merged_df['Matched COMPOUND_ID'], errors='coerce')\n",
    "merged_df['Correct COMPOUND_ID'] = pd.to_numeric(merged_df['Correct COMPOUND_ID'], errors='coerce')\n",
    "\n",
    "# extracting compounds that have no correct ChEBI ID and compounds that have a correct ChEBI ID (identified manually)\n",
    "df = merged_df[merged_df['Correct COMPOUND_ID'] != 0]\n",
    "df_2 = merged_df[merged_df['Correct COMPOUND_ID'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving as llm v2 results\n",
    "df.to_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_B/llm_subset_v2.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
