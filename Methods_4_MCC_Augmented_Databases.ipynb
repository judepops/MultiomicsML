{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import sklearn \n",
    "import sspa\n",
    "import sspa.utils\n",
    "import gseapy.plot as gp\n",
    "import networkx\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import urllib.request\n",
    "import statsmodels\n",
    "import networkx as nx\n",
    "import math\n",
    "import itertools \n",
    "from scipy.stats import hypergeom as hg\n",
    "import textwrap\n",
    "from itertools import chain\n",
    "import missforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, the standardd (filtered and not augmented) multi-omics database has to created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to download the kegg default pathways - selecting multiomics omics type which concatenates the two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_KEGG(organism, filepath=None, omics_type='metabolomics'):\n",
    "    '''\n",
    "    Function for KEGG pathway download\n",
    "    Args:\n",
    "        organism (str): KEGG 3 letter organism code\n",
    "        filepath (str): filepath to save pathway file to, default is None - save to variable\n",
    "    Returns: \n",
    "        GMT-like pd.DataFrame containing KEGG pathways\n",
    "    '''\n",
    "    print(\"Beginning KEGG download...\")\n",
    "    # get all pathways\n",
    "    url = 'http://rest.kegg.jp/list/pathway/'+organism\n",
    "    # change organism name\n",
    "    data = requests.get(url)\n",
    "    pathways = data.text\n",
    "    pathways = pathways.split(\"\\n\")\n",
    "    pathways = filter(None, pathways)\n",
    "    pathway_dict = dict()\n",
    "\n",
    "    for path in pathways:\n",
    "        path = path.split(\"\\t\")\n",
    "        name = path[1]\n",
    "        pathid = re.search(r\"(.*)\", path[0]).group(1)\n",
    "        pathway_dict[pathid] = name\n",
    "\n",
    "    # get compounds for each pathway\n",
    "    base_url = 'http://rest.kegg.jp/get/'\n",
    "\n",
    "    pathway_ids = [*pathway_dict]\n",
    "    pathway_names = list(pathway_dict.values())\n",
    "\n",
    "    # get release details\n",
    "    release_data = requests.get('http://rest.kegg.jp/info/kegg')\n",
    "    version_no = release_data.text.split()[9][0:3]\n",
    "\n",
    "    if omics_type == 'metabolomics':\n",
    "        pathway_compound_mapping = dict()\n",
    "\n",
    "        for index,i in enumerate(tqdm(pathway_ids)):\n",
    "            complist = []\n",
    "            current_url = base_url + \"pathway:\" +i\n",
    "            # parse the pathway description page\n",
    "            page = requests.get(current_url)\n",
    "            lines = page.text.split(\"\\n\")\n",
    "\n",
    "            try:\n",
    "                cpds_start = [lines.index(i) for i in lines if i.startswith(\"COMPOUND\")][0]\n",
    "                reference_start = [lines.index(i) for i in lines if i.startswith(\"REFERENCE\") or i.startswith(\"REL_PATHWAY\")][0]\n",
    "                cpds_lines = lines[cpds_start:reference_start]\n",
    "                first_cpd = cpds_lines.pop(0).split()[1]\n",
    "                complist.append(first_cpd)\n",
    "                complist = complist + [i.split()[0] for i in cpds_lines]\n",
    "                pathway_compound_mapping[i] = list(set(complist))\n",
    "            except IndexError:\n",
    "                pathway_compound_mapping[i] = []\n",
    "\n",
    "        # remove empty pathway entries\n",
    "        pathway_compound_mapping = {k: v for k, v in pathway_compound_mapping.items() if v}\n",
    "\n",
    "        # create GMT style file\n",
    "        df = pd.DataFrame.from_dict(pathway_compound_mapping, orient='index')\n",
    "        # map pathway names onto first column\n",
    "        df.insert(0, 'Pathway_name', df.index.map(pathway_dict.get))\n",
    "\n",
    "        if filepath:\n",
    "            fpath = filepath + \"/KEGG_\" + organism + \"_pathways_compounds_R\" + str(version_no) + \".gmt\"\n",
    "            df.to_csv(fpath, sep=\"\\t\", header=False)\n",
    "            print(\"KEGG DB file saved to \" + fpath)\n",
    "        print(\"Complete!\")\n",
    "\n",
    "        return df\n",
    "        \n",
    "\n",
    "    if omics_type == 'multiomics':\n",
    "        pathway_mapping = dict()\n",
    "\n",
    "        for index,i in enumerate(tqdm(pathway_ids)):\n",
    "            complist = []\n",
    "            genelist = []\n",
    "            current_url = base_url + \"pathway:\" +i\n",
    "            # parse the pathway description page\n",
    "            page = requests.get(current_url)\n",
    "            lines = page.text.split(\"\\n\")\n",
    "\n",
    "            try:\n",
    "                genes_start = [lines.index(i) for i in lines if i.startswith(\"GENE\")][0]\n",
    "                cpds_start = [lines.index(i) for i in lines if i.startswith(\"COMPOUND\")][0]\n",
    "                reference_start = [lines.index(i) for i in lines if i.startswith(\"REFERENCE\") or i.startswith(\"REL_PATHWAY\")][0]\n",
    "                genes_lines = lines[genes_start:cpds_start]\n",
    "                cpds_lines = lines[cpds_start:reference_start]\n",
    "\n",
    "                first_cpd = cpds_lines.pop(0).split()[1]\n",
    "                complist.append(first_cpd)\n",
    "                complist = complist + [i.split()[0] for i in cpds_lines]\n",
    "                first_gene = genes_lines.pop(0).split()[1]\n",
    "                genelist.append(first_gene)\n",
    "                genelist = genelist + [i.split()[0] for i in genes_lines]\n",
    "                pathway_mapping[i] = list(set(complist)) + list(set(genelist))\n",
    "            except IndexError:\n",
    "                pathway_mapping[i] = []\n",
    "\n",
    "        # remove empty pathway entries\n",
    "        pathway_mapping = {k: v for k, v in pathway_mapping.items() if v}\n",
    "\n",
    "        # create GMT style file\n",
    "        df = pd.DataFrame.from_dict(pathway_mapping, orient='index')\n",
    "        # map pathway names onto first column\n",
    "        df.insert(0, 'Pathway_name', df.index.map(pathway_dict.get))\n",
    "\n",
    "        if filepath:\n",
    "            fpath = filepath + \"/KEGG_\" + organism + \"_pathways_multiomics_R\" + str(version_no) + \".gmt\"\n",
    "            df.to_csv(fpath, sep=\"\\t\", header=False)\n",
    "            print(\"KEGG DB file saved to \" + fpath)\n",
    "        print(\"Complete!\")\n",
    "\n",
    "        return df\n",
    "        \n",
    "kegg_pathways_default= download_KEGG(organism = 'hsa', omics_type = 'multiomics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and saving the default multi-omics database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing hsa from row names - our pathways in augmented dataset dont have same codes but no hsa string at the start and we want them to be comparable\n",
    "kegg_pathways_default = kegg_pathways_default.astype(str)\n",
    "kegg_pathways_default.index = kegg_pathways_default.index.str.replace('hsa', '')\n",
    "\n",
    "# droping columns that are  all na\n",
    "kegg_pathways_default.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# resetting the index and renamoing index column to 'Pathway'\n",
    "kegg_pathways_default.reset_index(inplace=True)\n",
    "kegg_pathways_default.rename(columns={'index': 'Pathway'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_pathways_default.to_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_G/KEGG_database_multiomics_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating in augmented pathway databases by inverting and manipulating the JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON file format is all compound values annotated to (predicted) for each pathway key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "# Reading in the metabolomics data\n",
    "metabolomics_data_processed = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Processing/Processing_Cleaned/cleaned_metabolomics_data_covid.csv')\n",
    "metabolomics_data_processed = metabolomics_data_processed.set_index('sample_id')\n",
    "\n",
    "\n",
    "# removing Columns\n",
    "metabolomics_data_processed_final = metabolomics_data_processed.iloc[:, :-7]\n",
    "metabolomics_data_processed_final.columns = [col.strip().lower() for col in metabolomics_data_processed_final.columns]\n",
    "metabolomics_data_processed_final\n",
    "\n",
    "# kegg id Mappings\n",
    "manual = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_G/final_kegg_id.csv', index_col=0)\n",
    "manual\n",
    "\n",
    "# mapped data\n",
    "processed_data_mapped_manual = sspa.map_identifiers(manual, output_id_type=\"KEGG\", matrix=metabolomics_data_processed_final)\n",
    "processed_data_mapped_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the default kegg pathway multiomics database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathways\n",
    "\n",
    "filtered_kegg_pathways = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_G/KEGG_database_multiomics_filtered.csv', dtype={'Pathway': str}, index_col=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### level 2 kegg pathways (broad) were just given their name so they were manually mapped to the ID. ALL granular pathways had their IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) extracting the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading JSON data from the file provided by Huckvale et al\n",
    "file_path = '/Users/judepops/Documents/PathIntegrate/Code/Pathway_Prediction/Huckvale/kegg_all-pathway-mappings.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    kegg_data = json.load(file)\n",
    "\n",
    "# key for mapping level 2 kegg pathway names to their ids\n",
    "pathway_key = {\n",
    "    \"Metabolism\": \"09100\",\n",
    "    \"Carbohydrate metabolism\": \"09101\",\n",
    "    \"Energy metabolism\": \"09102\",\n",
    "    \"Lipid metabolism\": \"09103\",\n",
    "    \"Nucleotide metabolism\": \"09104\",\n",
    "    \"Amino acid metabolism\": \"09105\",\n",
    "    \"Metabolism of other amino acids\": \"09106\",\n",
    "    \"Glycan biosynthesis and metabolism\": \"09107\",\n",
    "    \"Metabolism of cofactors and vitamins\": \"09108\",\n",
    "    \"Metabolism of terpenoids and polyketides\": \"09109\",\n",
    "    \"Biosynthesis of other secondary metabolites\": \"09110\",\n",
    "    \"Xenobiotics biodegradation and metabolism\": \"09111\",\n",
    "    \"Not included in regular maps\": \"09112\",\n",
    "    \"Chemical structure transformation maps\": \"09120\",\n",
    "}\n",
    "\n",
    "# intialising a dictionary to store pathway data extracted\n",
    "pathway_dict = {}\n",
    "\n",
    "# amending JSON keys with the missing codes from teh dictionary\n",
    "for cpd, pathways in kegg_data.items():\n",
    "    cpd_id = cpd.replace('cpd:', '')  # removing the the 'cpd:' prefix from the compounds\n",
    "    for pathway in pathways:\n",
    "        if '  ' in pathway:  # ensuring each pathway entry has the expected format\n",
    "            pathway_id, pathway_name = pathway.split('  ', 1)\n",
    "            if pathway_id not in pathway_dict:\n",
    "                pathway_dict[pathway_id] = {'Pathway_name': pathway_name, 'Compounds': []}\n",
    "            pathway_dict[pathway_id]['Compounds'].append(cpd_id)\n",
    "        else:\n",
    "            # attemptign to clean + handle improperly formatted entries\n",
    "            pathway = pathway.strip()\n",
    "            pathway_id = pathway_key.get(pathway, None)\n",
    "            if pathway_id:\n",
    "                if pathway_id not in pathway_dict:\n",
    "                    pathway_dict[pathway_id] = {'Pathway_name': pathway, 'Compounds': []}\n",
    "                pathway_dict[pathway_id]['Compounds'].append(cpd_id)\n",
    "\n",
    "# verifying the number of unique pathways identified\n",
    "print(len(pathway_dict))\n",
    "\n",
    "# converging the dictionary output to df\n",
    "pathway_list = []\n",
    "for pathway_id, data in pathway_dict.items():\n",
    "    row = [pathway_id, data['Pathway_name']] + data['Compounds']\n",
    "    pathway_list.append(row)\n",
    "\n",
    "# creatign columns with the max number of compounds in a pathway as the max\n",
    "max_compounds = max(len(data['Compounds']) for data in pathway_dict.values())\n",
    "columns = ['Pathway', 'Pathway_name'] + [f'Compound {i+1}' for i in range(max_compounds)]\n",
    "# creating the final pathway database dataframe\n",
    "df = pd.DataFrame(pathway_list, columns=columns)\n",
    "# filling in  missing entries w/ NaN to ensure consistent dataframe shape\n",
    "df = df.reindex(columns=columns, fill_value=pd.NA)\n",
    "# clearning the dataframe - storing pathways as strings so they dont change \n",
    "df['Pathway'] = df['Pathway'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting the information (MCC etc from a different file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "pathway_key = {\n",
    "    \"Carbohydrate metabolism\": \"09101\",\n",
    "    \"Energy metabolism\": \"09102\",\n",
    "    \"Lipid metabolism\": \"09103\",\n",
    "    \"Nucleotide metabolism\": \"09104\",\n",
    "    \"Amino acid metabolism\": \"09105\",\n",
    "    \"Metabolism of other amino acids\": \"09106\",\n",
    "    \"Glycan biosynthesis and metabolism\": \"09107\",\n",
    "    \"Metabolism of cofactors and vitamins\": \"09108\",\n",
    "    \"Metabolism of terpenoids and polyketides\": \"09109\",\n",
    "    \"Biosynthesis of other secondary metabolites\": \"09110\",\n",
    "    \"Xenobiotics biodegradation and metabolism\": \"09111\",\n",
    "    \"Chemical structure transformation maps\": \"09120\",\n",
    "}\n",
    "\n",
    "with open('/Users/judepops/Documents/PathIntegrate/Code/Pathway_Prediction/Huckvale/pathway-info.json', 'r') as file:\n",
    "    json_dict = json.load(file)\n",
    "\n",
    "updated_json_dict = {}\n",
    "for key, value in json_dict.items():\n",
    "    parts = key.split()\n",
    "    if parts[0].isdigit():\n",
    "        updated_json_dict[key] = value\n",
    "    else:\n",
    "        pathway_name = key.strip()\n",
    "        if pathway_name in pathway_key:\n",
    "            code = pathway_key[pathway_name]\n",
    "            new_key = f\"{code}  {pathway_name}\"\n",
    "            updated_json_dict[new_key] = value\n",
    "        else:\n",
    "            updated_json_dict[key] = value\n",
    "\n",
    "pathway_data = []\n",
    "for key, value in updated_json_dict.items():\n",
    "    pathway_number = key.split()[0]\n",
    "    mean_mcc = value['mean_mcc']\n",
    "    size = value['size']\n",
    "    pathway_data.append({'Pathway': pathway_number, 'mean_mcc': mean_mcc, 'size': size})\n",
    "\n",
    "json_df = pd.DataFrame(pathway_data)\n",
    "\n",
    "json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the augmented pathway dataframe with the MCC informaiton on the pathway column, creating a augmented dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(json_df, on='Pathway', how='left')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualsing the distribtuion of MCC in pathways in the merged df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "plt.style.use('default')\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 1, 21)\n",
    "hist, bin_edges = np.histogram(merged_df['mean_mcc'], bins=bins)\n",
    "\n",
    "bar_colors = []\n",
    "for value in bin_edges[:-1]:\n",
    "    if 0 <= value < 0.7:\n",
    "        bar_colors.append('#fb2e2a')  \n",
    "    else:\n",
    "        bar_colors.append('#2d8d33')  \n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), edgecolor='k', alpha=1, color=bar_colors, align='edge')\n",
    "red_patch = plt.Line2D([0], [0], color='#fb2e2a', lw=10, label='Predicted KEGG Pathways')\n",
    "green_patch = plt.Line2D([0], [0], color='#2d8d33', lw=10, label='Predicted KEGG Pathways')\n",
    "\n",
    "plt.xlabel('Mean MCC', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(False)  # removing the grid lines\n",
    "\n",
    "# increasing size of axis \n",
    "plt.tick_params(axis='both', which='major', labelsize=20)  # Major ticks\n",
    "plt.tick_params(axis='both', which='minor', labelsize=14)  # Minor ticks (if any)\n",
    "plt.legend(handles=[red_patch, green_patch], loc='upper left', fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now incorporating the MCC information and creating pathway cutoffs using MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only use predictions of at least 0.4 after 0 as tehre is not much change before then\n",
    "thresholds = [0.0, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# creating a dictionary to store filtered datasets\n",
    "filtered_datasets = {}\n",
    "\n",
    "# filtering the merged_df augmented pathway dataset pathways based on MCC thresholds and store in the dictionary\n",
    "for threshold in thresholds:\n",
    "    filtered_datasets[f'above_{threshold}'] = merged_df[merged_df['mean_mcc'] > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an info dataframe with different statistics for anlaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = []\n",
    "\n",
    "for threshold, dataset in filtered_datasets.items():\n",
    "    num_pathways = len(dataset)\n",
    "    num_rows, num_columns = dataset.shape\n",
    "    mcc_distribution = dataset['mean_mcc'].mean()\n",
    "    non_na_entries = dataset.notna().sum().sum()\n",
    "    \n",
    "    non_na_entries_unique = dataset.drop_duplicates().notna().sum().sum()\n",
    "    \n",
    "    info_list.append({\n",
    "        'threhsold': threshold,\n",
    "        'pathways count': num_pathways,\n",
    "        'rows count': num_rows,\n",
    "        'column count': num_columns,\n",
    "        'mean MCC': mcc_distribution,\n",
    "        'non-na count': non_na_entries_unique\n",
    "    })\n",
    "\n",
    "info_df = pd.DataFrame(info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN step: combinign the default dataframe to the MCC-filtered augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function tto combine the datasets - also tracks the row counts\n",
    "def combine_datasets(df, filtered_kegg_pathways, dataset):\n",
    "    dataset_pathways = set(dataset['Pathway'])\n",
    "    kegg_pathways = set(filtered_kegg_pathways['Pathway'])\n",
    "\n",
    "    unique_dataset_pathways = dataset_pathways - kegg_pathways\n",
    "    unique_kegg_pathways = kegg_pathways - dataset_pathways\n",
    "\n",
    "    matching_pathways = dataset_pathways.intersection(kegg_pathways)\n",
    "\n",
    "    combined_data = []\n",
    "    df_unique_count = 0\n",
    "    kegg_unique_count = 0\n",
    "\n",
    "    # combining the entries for matching pathways\n",
    "    for pathway in matching_pathways:\n",
    "        pathway_name = dataset[dataset['Pathway'] == pathway]['Pathway_name'].iloc[0]\n",
    "        df_entries = dataset[dataset['Pathway'] == pathway].drop(columns=['Pathway', 'Pathway_name']).values.flatten()\n",
    "        kegg_entries = filtered_kegg_pathways[filtered_kegg_pathways['Pathway'] == pathway].drop(columns=['Pathway', 'Pathway_name']).values.flatten()\n",
    "        combined_entries = list(set(df_entries) | set(kegg_entries))  \n",
    "        combined_data.append([pathway, pathway_name, 'Combined'] + combined_entries)\n",
    "\n",
    "    # processing the pathways that are unique to the original dataset (filtered kegg pathways)\n",
    "    for pathway in unique_dataset_pathways:\n",
    "        pathway_name = dataset[dataset['Pathway'] == pathway]['Pathway_name'].iloc[0]\n",
    "        df_entries = dataset[dataset['Pathway'] == pathway].drop(columns=['Pathway', 'Pathway_name']).values.flatten()\n",
    "        combined_data.append([pathway, pathway_name, 'df_unique'] + list(df_entries))\n",
    "        df_unique_count += 1\n",
    "\n",
    "    # processing pathways that are unique to the KEGG dataset (augmented - df)\n",
    "    for pathway in unique_kegg_pathways:\n",
    "        pathway_name = filtered_kegg_pathways[filtered_kegg_pathways['Pathway'] == pathway]['Pathway_name'].iloc[0]\n",
    "        kegg_entries = filtered_kegg_pathways[filtered_kegg_pathways['Pathway'] == pathway].drop(columns=['Pathway', 'Pathway_name']).values.flatten()\n",
    "        combined_data.append([pathway, pathway_name, 'kegg_unique'] + list(kegg_entries))\n",
    "        kegg_unique_count += 1\n",
    "\n",
    "    # calculating the maximum number of entries in any row (this allows creation of consistent columns)\n",
    "    max_entries = max(len(row) for row in combined_data)\n",
    "    columns = ['Pathway', 'Pathway_name', 'Source'] + [f'Entry {i+1}' for i in range(max_entries - 3)]\n",
    "    combined_df = pd.DataFrame(combined_data, columns=columns)\n",
    "    combined_df = combined_df.reindex(columns=columns, fill_value=pd.NA)\n",
    "\n",
    "    return combined_df, len(matching_pathways), df_unique_count, kegg_unique_count\n",
    "\n",
    "# dictionary to store the merged dataframes and their row counts \n",
    "merged_datasets = {}\n",
    "summary_data = []\n",
    "\n",
    "# combining each filtered dataset w/ filtered_kegg_pathways (the default dataset)\n",
    "# also calculating informaiton to store\n",
    "for threshold, dataset in filtered_datasets.items():\n",
    "    merged_df, matching_count, df_unique_count, kegg_unique_count = combine_datasets(df, filtered_kegg_pathways, dataset)\n",
    "    merged_datasets[threshold] = merged_df\n",
    "    non_na_entries = merged_df.notna().sum().sum()\n",
    "    non_na_entries_unique = merged_df.drop_duplicates().notna().sum().sum()\n",
    "\n",
    "    summary_data.append({\n",
    "        'threshold': threshold,\n",
    "        'row count': merged_df.shape[0],\n",
    "        'matching pathways': matching_count,\n",
    "        'Unique to df': df_unique_count,\n",
    "        'Unique to KEGG': kegg_unique_count,\n",
    "        'Non-NA Entries': non_na_entries,\n",
    "        'Non-NA Unique Entries': non_na_entries_unique\n",
    "    })\n",
    "\n",
    "# creating a summary dataframe for final results\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pathway analysis need to make sure that the Pathway column is set as the index for each dataset \n",
    "for key in merged_datasets:\n",
    "    merged_datasets[key] = merged_datasets[key].set_index('Pathway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathway analysis with normal kegg and augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loeading the default suathway data for the KEGG dataset\n",
    "mo_paths = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_G/KEGG_database_multiomics_filtered.csv', dtype={'Pathway': str}, index_col='Pathway')\n",
    "# saving this default non-augmented dataset as original\n",
    "mo_paths = mo_paths.drop(columns='Unnamed: 0')\n",
    "mo_paths = mo_paths.astype('object')\n",
    "merged_datasets['original'] = mo_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pathintegrate\n",
    "\n",
    "\n",
    "# function to train and cross validate the model - this is the same as before for consistiency\n",
    "def train_and_evaluate_model(random_seed, prot, metab, mo_paths, shuffle_labels=False):\n",
    "    X_train_prot, X_test_prot, y_train, y_test = train_test_split(\n",
    "        prot.drop(columns=['Condition_Group']), prot['Condition_Group'],\n",
    "        test_size=0.33, random_state=random_seed, stratify=prot['Condition_Group']\n",
    "    )\n",
    "\n",
    "    if shuffle_labels:\n",
    "        np.random.shuffle(y_train.values)\n",
    "        np.random.shuffle(y_test.values)\n",
    "\n",
    "    X_train_met, X_test_met = metab.loc[X_train_prot.index, :], metab.loc[X_test_prot.index, :]\n",
    "\n",
    "    pi_model = pathintegrate.PathIntegrate(\n",
    "        omics_data={'Metabolomics_train': X_train_met, 'Proteomics_train': X_train_prot},\n",
    "        metadata=y_train,\n",
    "        pathway_source=mo_paths,\n",
    "        sspa_scoring=sspa.sspa_SVD,\n",
    "        min_coverage=4\n",
    "    )\n",
    "\n",
    "    cv_single_view = pi_model.SingleViewCV(\n",
    "        LogisticRegression,\n",
    "        model_params={'random_state': 0, 'max_iter': 500},\n",
    "        cv_params={'cv': 5, 'scoring': 'f1', 'verbose': 2}\n",
    "    )\n",
    "\n",
    "    print('Mean cross-validated F1 score: ', np.mean(cv_single_view))\n",
    "\n",
    "    sv_tuned = pi_model.SingleView(\n",
    "        model=LogisticRegression,\n",
    "        model_params={'C': 21.54434690031882, 'random_state': 0, 'max_iter': 500}\n",
    "    )\n",
    "\n",
    "    concat_data = pd.concat({'Metabolomics_test': X_test_met, 'Proteomics_test': X_test_prot.iloc[:, :-1]}.values(), axis=1)\n",
    "\n",
    "    pipe_sv = Pipeline([\n",
    "        ('Scaler', StandardScaler().set_output(transform=\"pandas\")),\n",
    "        ('sspa', pi_model.sspa_method(pi_model.pathway_source, pi_model.min_coverage)),\n",
    "    ])\n",
    "\n",
    "    test_set_scores = pipe_sv.fit_transform(concat_data)\n",
    "\n",
    "    sv_pred = sv_tuned.predict(test_set_scores)\n",
    "    sv_pred_prob = sv_tuned.predict_proba(test_set_scores)[:, 1]\n",
    "\n",
    "    test_set_f1 = f1_score(y_test, sv_pred)\n",
    "    test_set_precision = precision_score(y_test, sv_pred)\n",
    "    test_set_recall = recall_score(y_test, sv_pred)\n",
    "    test_set_auc = roc_auc_score(y_test, sv_pred_prob)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, sv_pred_prob)\n",
    "\n",
    "    return test_set_f1, test_set_precision, test_set_recall, test_set_auc, fpr, tpr, random_seed\n",
    "\n",
    "# function to run the model for all the seeds and calcualte an  average ROC - this is also the same as before\n",
    "def run_model_for_all_seeds(random_seeds, prot, metab, mo_paths, shuffle_labels=False):\n",
    "    num_runs = len(random_seeds)\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    auc_scores = []\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    all_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        random_seed = random_seeds[i]\n",
    "        f1, precision, recall, auc, fpr, tpr, used_seed = train_and_evaluate_model(random_seed, prot, metab, mo_paths, shuffle_labels)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        auc_scores.append(auc)\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(interpolate.interp1d(fpr, tpr)(all_fpr))\n",
    "        print(f\"Run {i + 1}: F1 = {f1}, Precision = {precision}, Recall = {recall}, AUC = {auc}, Seed = {used_seed}\")\n",
    "\n",
    "    mean_tpr = np.mean(tpr_list, axis=0)\n",
    "    std_tpr = np.std(tpr_list, axis=0)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "\n",
    "    return all_fpr, mean_tpr, mean_auc, std_tpr, std_auc\n",
    "\n",
    "# loading the random seeds --> 50 random seeds and selecting 20\n",
    "random_seeds = np.random.randint(50, size=20)\n",
    "\n",
    "# Load datasets for KEGG\n",
    "metab = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_G/COVID_Met_KEGG_Pred.csv')\n",
    "prot = pd.read_csv('/Users/judepops/Documents/PathIntegrate/Code/Final_Scripts/Results/Results_G/COVID_Prot_KEGG_Final.csv')\n",
    "prot.set_index('sample_id', inplace=True)\n",
    "metab.set_index('sample_id', inplace=True)\n",
    "prot = prot.drop(columns=['Who', 'Race', 'Age', 'Group', 'Age_Group', 'Race_Group'])\n",
    "metab = metab.drop(columns=['Who', 'Race', 'Age', 'Group', 'Age_Group', 'Race_Group'])\n",
    "common_indices = prot.index.intersection(metab.index)\n",
    "prot = prot.loc[common_indices]\n",
    "metab = metab.loc[common_indices]\n",
    "metab = metab.iloc[:, :-1]\n",
    "\n",
    "prot['Condition_Group'] = prot['Condition_Group'].map({'Severe': 1, 'Mild': 0})\n",
    "\n",
    "\n",
    "# creating a diciontary a to store roc results (and standard deviation) for each threshold\n",
    "roc_results = {}\n",
    "\n",
    "# runnign the models for each threshold dataset and collecting the roc results\n",
    "for threshold, dataset in merged_datasets.items():\n",
    "    fpr, mean_tpr, mean_auc, std_tpr, std_auc = run_model_for_all_seeds(random_seeds, prot, metab, dataset)\n",
    "    roc_results[threshold] = (fpr, mean_tpr, mean_auc, std_tpr, std_auc)\n",
    "    print(F'Threshold {threshold} has finished')\n",
    "    print(F'The AUC is: {mean_auc}')\n",
    "\n",
    "# ploitting unique roc curves for each threshold in the augmented dataset\n",
    "plt.figure()\n",
    "\n",
    "# a null model is also created for  ROC comparison\n",
    "fpr_null, mean_tpr_null, mean_auc_null, std_tpr_null, std_auc_null = run_model_for_all_seeds(random_seeds, prot, metab, mo_paths, shuffle_labels=True)\n",
    "plt.plot(fpr_null, mean_tpr_null, color='black', linestyle='--', label=f'Null Model ROC curve (area = {mean_auc_null:.2f})')\n",
    "\n",
    "for threshold, (fpr, mean_tpr, mean_auc, std_tpr, std_auc) in roc_results.items():\n",
    "    plt.plot(fpr, mean_tpr, label=f'Threshold {threshold} ROC curve (area = {mean_auc:.2f}, std = {std_auc:.2f})')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('roc curves for diff thresholds')\n",
    "plt.legend(loc=\"lower right\", prop={'size': 8})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a function to ensure that the roc  starts at the original\n",
    "def ensure_start_at_zero(fpr, tpr):\n",
    "    if fpr[0] != 0 or tpr[0] != 0:\n",
    "        fpr = np.insert(fpr, 0, 0)\n",
    "        tpr = np.insert(tpr, 0, 0)\n",
    "    return fpr, tpr\n",
    "\n",
    "# fucntion that is created to make sure values are within ranges\n",
    "def clip_to_range(arr):\n",
    "    return np.clip(arr, 0, 1)\n",
    "fpr_null, mean_tpr_null = ensure_start_at_zero(fpr_null, mean_tpr_null)\n",
    "fpr_null, mean_tpr_null = clip_to_range(fpr_null), clip_to_range(mean_tpr_null)\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "\n",
    "plt.figure()\n",
    "colors = {\n",
    "    'original': 'blue',\n",
    "    'above_0.7': 'green',\n",
    "    'above_0.0': 'red'\n",
    "}\n",
    "\n",
    "def equalize_lengths(fpr, mean_tpr, std_tpr):\n",
    "    min_len = min(len(fpr), len(mean_tpr), len(std_tpr))\n",
    "    return fpr[:min_len], mean_tpr[:min_len], std_tpr[:min_len]\n",
    "for threshold, (fpr, mean_tpr, mean_auc, std_tpr, std_auc) in roc_results.items():\n",
    "    if threshold in colors:\n",
    "        color = colors[threshold]\n",
    "        fpr, mean_tpr = ensure_start_at_zero(fpr, mean_tpr)\n",
    "        fpr, mean_tpr, std_tpr = equalize_lengths(fpr, mean_tpr, std_tpr)\n",
    "        fpr, mean_tpr, std_tpr = clip_to_range(fpr), clip_to_range(mean_tpr), clip_to_range(std_tpr)\n",
    "        plt.step(fpr, mean_tpr, where='post', color=color, label=f'Threshold {threshold} (area = {mean_auc:.3f}, std = {std_auc:.2f})')\n",
    "        plt.fill_between(fpr, clip_to_range(mean_tpr - std_tpr), clip_to_range(mean_tpr + std_tpr), color=color, alpha=0.2, label=f'Threshold {threshold} ±1 std dev')\n",
    "\n",
    "plt.step(fpr_null, mean_tpr_null, where='post', color='black', linestyle='--', label=f'Null Model ROC curve (area = {mean_auc_null:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('roc curves for diff thresholds')\n",
    "plt.legend(loc=\"lower right\", prop={'size': 8})\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)  # Major ticks\n",
    "plt.tick_params(axis='both', which='minor', labelsize=15)  # Minor ticks (if any)\n",
    "plt.savefig('/Users/judepops/Documents/PathIntegrate/Code/Pathway_Prediction/Huckvale/Results_2/roc_key_thresholds.png', dpi=500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
